{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from typing import Annotated, Literal, TypedDict, Sequence, Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, AnyMessage\n",
    "\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils.memory_store import MemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = MemoryStore()\n",
    "USER_ID = \"3\"\n",
    "key = \"semantic_memory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    personal_info_detected: Literal[\"yes\", \"no\"]\n",
    "    delete_request: Literal[\"yes\", \"no\"]\n",
    "    personal_info_extracted: Optional[List[str]] # this stores entity for either to delete or add\n",
    "    new_info: Optional[str]\n",
    "    collected_memories: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyInformation(BaseModel):\n",
    "    personal_info: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Indicates whether the information contains any personal user choices or preferences that could help an LLM provide more personalized responses over time for long-term memory use.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info_classifier(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Classifies if the last user message contains personal info.\n",
    "    Now also detects if user is forgetting or updating past preferences.\n",
    "    \"\"\"\n",
    "    message = state[\"messages\"][-1].content\n",
    "\n",
    "    system_prompt = \"\"\"You are a classifier that checks if a message contains personal info.\n",
    "\n",
    "Personal info includes:\n",
    "- Names (e.g., \"John Smith\")\n",
    "- Locations (e.g., \"Berlin\", \"123 Main St\")\n",
    "- Preferences or hobbies (e.g., \"I love to code\", \"I prefer short replies\")\n",
    "- Tools and Technologies Used  (e.g., \"I mostly use PyTorch and FastAPI\", \"I am using Windows 11 now\")\n",
    "- Dislikes or updates to previous preferences (e.g., \"I do not use Magnet anymore\", \"I stopped liking Twitter\")\n",
    "- Occupation\n",
    "\n",
    "Respond \"yes\" if the message reveals or updates any personal preferences, locations, names, etc.\n",
    "Respond \"no\" if it is a general query or lacks any personal information.\n",
    "\n",
    "Examples:\n",
    "User: \"My name is Thomas, I live in Vancouver.\"\n",
    "Classifier: \"yes\"\n",
    "\n",
    "User: \"I love pizza with extra cheese.\"\n",
    "Classifier: \"yes\"\n",
    "\n",
    "User: \"I no longer use Discord.\"\n",
    "Classifier: \"yes\"\n",
    "\n",
    "User: \"I do not like tea anymore.\"\n",
    "Classifier: \"yes\"\n",
    "\n",
    "User: \"I still use Notion daily.\"\n",
    "Classifier: \"yes\"\n",
    "\n",
    "User: \"What is the capital of France?\"\n",
    "Classifier: \"no\"\n",
    "\n",
    "User: \"This is great weather.\"\n",
    "Classifier: \"no\"\n",
    "\n",
    "User: \"Hello, how are you?\"\n",
    "Classifier: \"no\"\n",
    "\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ])\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=50)\n",
    "    structured_llm = llm.with_structured_output(ClassifyInformation)\n",
    "    chain = prompt | structured_llm\n",
    "\n",
    "    result = chain.invoke({\"message\": message})\n",
    "    state[\"personal_info_detected\"] = result.personal_info\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info_router(state: AgentState) -> Literal[\"classify_add_or_delete\", \"retrieve_memories\"]:\n",
    "    \"\"\"\n",
    "    If personal info is detected, route to the node that classifies whether to add or delete it.\n",
    "    Otherwise, route to retrieving memories.\n",
    "    \"\"\"\n",
    "    if state[\"personal_info_detected\"].lower() == \"yes\":\n",
    "        return \"classify_add_or_delete\"\n",
    "    return \"retrieve_memories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeleteRequest(BaseModel):\n",
    "    delete_request: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Return 'yes' if the user wants to delete or stop using something previously shared, otherwise 'no'.\"\n",
    "    )\n",
    "\n",
    "def classify_add_or_delete(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Classifies whether the personal information is a delete request or an addition.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    # print(f\"last message: {last_message}\")\n",
    "\n",
    "    system_prompt = \"\"\"You are a classifier that decides whether a user's message indicates a request to delete or stop using something.\n",
    "\n",
    "    Return \"yes\" if the user says they:\n",
    "    - No longer use something\n",
    "    - Have stopped liking a tool, app, or preference\n",
    "    - Want to remove or undo a previously stated preference\n",
    "\n",
    "    Return \"no\" if the user is sharing a new preference, habit, tool, or hobby.\n",
    "\n",
    "    Examples:\n",
    "    User: \"I do not use Facebook anymore.\"\n",
    "    Classifier: \"yes\"\n",
    "\n",
    "    User: \"I stopped liking pineapple on pizza.\"\n",
    "    Classifier: \"yes\"\n",
    "\n",
    "    User: \"I enjoy hiking on weekends.\"\n",
    "    Classifier: \"no\"\n",
    "\n",
    "    User: \"I love using Notion for productivity.\"\n",
    "    Classifier: \"no\"\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", last_message),\n",
    "    ])\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=50)\n",
    "    structured_llm = llm.with_structured_output(DeleteRequest)\n",
    "    chain = prompt | structured_llm\n",
    "\n",
    "    result = chain.invoke({\"message\": prompt})\n",
    "    # print(f\"Result: {result}\")\n",
    "    state[\"delete_request\"] = result.delete_request\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"hi, I don't want to use magnet anymore\", additional_kwargs={}, response_metadata={})],\n",
       " 'delete_request': 'yes'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_add_or_delete({\"messages\": [HumanMessage(content=\"hi, I don't want to use magnet anymore\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_add_or_delete(state: AgentState) -> Literal[\"extract_delete_entity\", \"personal_info_extractor\"]:\n",
    "    \"\"\"\n",
    "    If user intends to delete info, route to extract_delete_entity.\n",
    "    Otherwise, route to personal_info_extractor for new additions.\n",
    "    \"\"\"\n",
    "    if state[\"delete_request\"].lower() == \"yes\":\n",
    "        return \"extract_delete_entity\"\n",
    "    return \"personal_info_extractor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an assistant that extracts the names of tools, preferences, or technologies the user wants to forget or delete from memory. Only return the list of entities to remove.\"\"\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"input\": \"I no longer use Twitter.\",\n",
    "        \"output\": '[\"Twitter\"]'\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Forget Notion and Todoist.\",\n",
    "        \"output\": '[\"Notion\", \"Todoist\"]'\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I stopped liking pizza and burgers.\",\n",
    "        \"output\": '[\"pizza\", \"burgers\"]'\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I do not use Figma, Zoom, or Slack anymore.\",\n",
    "        \"output\": '[\"Figma\", \"Zoom\", \"Slack\"]'\n",
    "    },\n",
    "]\n",
    "    \n",
    "def format_prompt(user_message: str) -> List:\n",
    "    few_shots = \"\\n\".join(\n",
    "        [f\"Input: {ex['input']}\\nOutput: {ex['output']}\" for ex in few_shot_examples]\n",
    "    )\n",
    "    return [\n",
    "        SystemMessage(content=system_prompt + \"\\n\\n\" + few_shots),\n",
    "        HumanMessage(content=f\"Input: {user_message}\\nOutput:\")\n",
    "    ]\n",
    "\n",
    "class EntitiesToForget(BaseModel):\n",
    "    entities: List[str] = Field(..., description=\"List of entities the user wants to delete or forget\")\n",
    "\n",
    "def extract_delete_entity(state: AgentState) -> AgentState:\n",
    "    messages = state[\"messages\"][-1].content\n",
    "    user_input = messages # Get last user message content\n",
    "\n",
    "    # System instructions with few-shot examples\n",
    "    instruction = \"\"\"\n",
    "    You are an intelligent assistant that helps identify which pieces of personal information the user wants to delete from memory.\n",
    "\n",
    "    Instructions:\n",
    "    - Analyze the input and extract the names of entities, preferences, or facts the user wants the system to forget.\n",
    "    - Return only a list of strings, each representing one such item.\n",
    "\n",
    "    Examples:\n",
    "    User: \"Forget that I live in Delhi and that I work at Microsoft.\"\n",
    "    Output: [\"Location: Delhi\", \"Employer: Microsoft\"]\n",
    "\n",
    "    User: \"Remove everything about my cat and my love for sushi.\"\n",
    "    Output: [\"Pet: cat\", \"Food Preference: sushi\"]\n",
    "\n",
    "    User: \"Never store my email or my travel plans to Japan.\"\n",
    "    Output: [\"Email\", \"Travel Plan: Japan\"]\n",
    "\n",
    "    User: \"Just chatting, nothing to delete.\"\n",
    "    Output: []\n",
    "\n",
    "    Now extract the entities from the following user input:\n",
    "    \"\"\" + user_input\n",
    "\n",
    "    # Initialize the structured model\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=20)\n",
    "    structured_llm = llm.with_structured_output(EntitiesToForget)\n",
    "\n",
    "    try:\n",
    "        response = structured_llm.invoke(instruction)\n",
    "        extracted = response.entities\n",
    "    except Exception:\n",
    "        extracted = []\n",
    "\n",
    "    # print(\"Entities to delete:\", extracted)\n",
    "    return {\n",
    "        \"personal_info_extracted\": extracted\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'personal_info_extracted': ['Communication Preference: Slack']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_delete_entity({\"messages\": [HumanMessage(content= \"I don't like to use Slack\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forget_logic(state: AgentState) -> AgentState:\n",
    "    entities = [item for item in state['personal_info_extracted']]\n",
    "    namespace = (\"user\", USER_ID)\n",
    "    key = \"semantic_memory\"\n",
    "    print(f\"Deleting these entities: {entities}\")\n",
    "    store.delete(namespace, key, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entities(BaseModel):\n",
    "    entities: List[str] = Field(..., description=\"List of entities the user wants to add to memory\")\n",
    "\n",
    "def personal_info_extractor(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Extracts personal info from the user's message using few-shot examples.\n",
    "    \"\"\"\n",
    "    message = state[\"messages\"][-1].content\n",
    "\n",
    "    # A few-shot style system prompt for extraction:\n",
    "    extractor_prompt = \"\"\"\n",
    "    You are an intelligent extractor that identifies and summarizes personal user information for long-term memory storage. Your goal is to help the LLM provide more personalized responses in the future.\n",
    "\n",
    "    Instructions:\n",
    "    - Read the user's input carefully.\n",
    "    - Extract any relevant personal details, such as:\n",
    "    - Name, location, profession, and age\n",
    "    - Interests, hobbies, goals, future plans\n",
    "    - Food preferences (likes/dislikes)\n",
    "    - Emotional states, values, or beliefs\n",
    "    - Mention of past experiences (e.g., education, travel)\n",
    "    - Any unique or identifying details\n",
    "\n",
    "    Output Format:\n",
    "    - Return a concise, comma-separated list of attributes in the format: \n",
    "    \"Attribute: Value\"\n",
    "    - If multiple values exist, separate them with commas.\n",
    "    - If no personal info is found, return: \"No personal info found.\"\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    User: \"I am John and I live in Seattle.\"\n",
    "    Output: [\"Name: John\", \"Location: Seattle\"]\n",
    "\n",
    "    User: \"Hey, I'm Lucy. I love eating bananas!\"\n",
    "    Output: [\"Name: Lucy\", \"Food Preference: bananas\"]\n",
    "\n",
    "    User: \"I am planning to apply to grad school in Germany next year.\"\n",
    "    Output: [\"Goal: apply to grad school, Germany\"]\n",
    "\n",
    "    User: \"Just a random statement about the weather.\"\n",
    "    Output: []\n",
    "\n",
    "    Now extract personal information from the following user input:\n",
    "    \"\"\" + message\n",
    "   \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=20)\n",
    "    structured_llm = llm.with_structured_output(Entities)\n",
    "\n",
    "    try:\n",
    "        response = structured_llm.invoke(extractor_prompt)\n",
    "        # print(response)\n",
    "        extracted = response.entities\n",
    "    except Exception:\n",
    "        extracted = []\n",
    "\n",
    "    # print(\"Entities to Add:\", extracted)\n",
    "\n",
    "\n",
    "    state[\"personal_info_extracted\"] = extracted\n",
    "    # print(state[\"personal_info_extracted\"])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi, I am Ashutosh. I love pizzas', additional_kwargs={}, response_metadata={})],\n",
       " 'personal_info_extracted': ['Name: Ashutosh', 'Food Preference: pizzas']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personal_info_extractor({\"messages\": [HumanMessage(content=\"hi, I am Ashutosh. I love pizzas\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNoveltyGrade(BaseModel):\n",
    "    score: Literal[\"yes\", \"no\"] = Field()\n",
    "\n",
    "def personal_info_duplicate_classifier(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Checks if the newly extracted info is already in the store or not.\n",
    "    If 'Yes', it's new info. If 'No', it's a duplicate.\n",
    "    \"\"\"\n",
    "    new_info = state.get(\"personal_info_extracted\", \"\")\n",
    "    namespace = (\"user\", USER_ID)\n",
    "    key = \"semantic_memory\"\n",
    "    results = store.get(namespace, key)\n",
    "    old_info_list = [doc for doc in results]\n",
    "\n",
    "    system_msg = \"\"\"You are a classifier that checks if the new personal info is already stored.\n",
    "If the new info adds anything new, respond 'Yes'. Otherwise 'No'.\"\"\"\n",
    "\n",
    "    old_info_str = \"\\n\".join(old_info_list) if old_info_list else \"No stored info so far.\"\n",
    "    human_template = \"\"\"New info:\\n{new_info}\\n\n",
    "Existing memory:\\n{old_info}\\n\n",
    "Answer ONLY 'Yes' if the new info is unique. Otherwise 'No'.\"\"\"\n",
    "    human_msg = human_template.format(new_info=new_info, old_info=old_info_str)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_msg),\n",
    "            (\"human\", \"{human_msg}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=50).with_structured_output(InfoNoveltyGrade)\n",
    "    chain = prompt | llm\n",
    "    result = chain.invoke({\"human_msg\": human_msg})\n",
    "    state[\"new_info\"] = result.score.strip()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info_deduper_router(state: AgentState) -> Literal[\"personal_info_storer\", \"retrieve_memories\"]:\n",
    "    \"\"\"\n",
    "    If 'Yes', store the new info. Otherwise skip storing.\n",
    "    \"\"\"\n",
    "    if state[\"new_info\"].lower() == \"yes\":\n",
    "        return \"personal_info_storer\"\n",
    "    return \"retrieve_memories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info_storer(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Stores the new personal info in memory if it exists.\n",
    "    \"\"\"\n",
    "    extracted = state.get(\"personal_info_extracted\")\n",
    "    \n",
    "    if extracted:\n",
    "        namespace = (\"user\", USER_ID)\n",
    "        store.put(namespace, key, {\"text\": extracted})\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_memories(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Retrieves all personal info from the store and aggregates into 'collected_memories'.\n",
    "    \"\"\"\n",
    "    results = store.get((\"user\", USER_ID), key)\n",
    "    memory_strs = [doc for doc in results]\n",
    "    state[\"collected_memories\"] = \"\\n\".join(memory_strs)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_personal_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Logs the memory to stdout for debugging (optional).\n",
    "    \"\"\"\n",
    "    print(\"----- Logging Personal Memory -----\")\n",
    "    if state[\"collected_memories\"]:\n",
    "        for i, line in enumerate(state[\"collected_memories\"].split(\"\\n\"), start=1):\n",
    "            print(f\"[Memory {i}] {line}\")\n",
    "    else:\n",
    "        print(\"[Memory] No personal info stored yet.\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Final LLM call that uses the collected memories in a SystemMessage.\n",
    "    \"\"\"\n",
    "    personal_info = state.get(\"collected_memories\", \"\")\n",
    "    system_msg = SystemMessage(\n",
    "        content=f\"You are a helpful assistant. The user has shared these personal details:\\n{personal_info}\"\n",
    "    )\n",
    "    all_messages = [system_msg] + list(state[\"messages\"])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=50)\n",
    "    response = llm.invoke(all_messages)\n",
    "    state[\"messages\"] = state[\"messages\"] + [response]\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"personal_info_classifier\", personal_info_classifier)\n",
    "workflow.add_node(\"classify_add_or_delete\", classify_add_or_delete)\n",
    "workflow.add_node(\"extract_delete_entity\", extract_delete_entity)\n",
    "workflow.add_node(\"forget_logic\", forget_logic)\n",
    "workflow.add_node(\"personal_info_extractor\", personal_info_extractor)\n",
    "workflow.add_node(\"personal_info_duplicate_classifier\", personal_info_duplicate_classifier)\n",
    "workflow.add_node(\"personal_info_storer\", personal_info_storer)\n",
    "workflow.add_node(\"retrieve_memories\", retrieve_memories)\n",
    "workflow.add_node(\"log_personal_memory\", log_personal_memory)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "\n",
    "workflow.add_edge(START, \"personal_info_classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"personal_info_classifier\",\n",
    "    personal_info_router,\n",
    "    {\n",
    "        \"classify_add_or_delete\": \"classify_add_or_delete\",\n",
    "        \"retrieve_memories\": \"retrieve_memories\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_add_or_delete\",\n",
    "    route_add_or_delete,\n",
    "    {\n",
    "        \"extract_delete_entity\": \"extract_delete_entity\",\n",
    "        \"personal_info_extractor\": \"personal_info_extractor\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"personal_info_extractor\", \"personal_info_duplicate_classifier\")\n",
    "workflow.add_edge(\"extract_delete_entity\",\"forget_logic\")\n",
    "workflow.add_edge(\"forget_logic\", \"retrieve_memories\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"personal_info_duplicate_classifier\",\n",
    "    personal_info_deduper_router,\n",
    "    {\n",
    "        \"personal_info_storer\": \"personal_info_storer\",\n",
    "        \"retrieve_memories\": \"retrieve_memories\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"personal_info_storer\", \"retrieve_memories\")\n",
    "workflow.add_edge(\"retrieve_memories\", \"log_personal_memory\")\n",
    "workflow.add_edge(\"log_personal_memory\", \"call_model\")\n",
    "workflow.add_edge(\"call_model\", END)\n",
    "\n",
    "# workflow.set_entry_point(\"personal_info_classifier\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "# graph = workflow.compile(checkpointer=checkpointer, store=store)\n",
    "graph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting these entities: ['Job Title: AI Engineer']\n",
      "----- Logging Personal Memory -----\n",
      "[Memory 1] Name: Ashutosh\n"
     ]
    }
   ],
   "source": [
    "# input_data_1 = {\"messages\": [HumanMessage(content=\"Hi, I am Ashutosh. I am an AI Engineer\")]}\n",
    "input_data_1 = {\"messages\": [HumanMessage(content=\"I am no longer an AI Engineer\")]}\n",
    "response = graph.invoke(input=input_data_1, config={\"configurable\": {\"thread_id\": 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi, I am Ashutosh. I am an AI Engineer', additional_kwargs={}, response_metadata={}, id='0278f460-c38f-4c67-938f-113f0e93b646'),\n",
       "  AIMessage(content=\"Hello, Ashutosh! It's great to meet you. As an AI Engineer, you must be working on some interesting projects. What specific areas of AI do you focus on?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 48, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BzqvtSdUfOafljFKzBcaG6eSsUgep', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ad861cc3-1dad-4497-8671-d3b6894cb5b5-0', usage_metadata={'input_tokens': 48, 'output_tokens': 36, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='I am no longer an AI Engineer', additional_kwargs={}, response_metadata={}, id='b8b0233e-ab57-449e-8c75-ca2694bab151'),\n",
       "  AIMessage(content='Got it, Ashutosh! If you’d like to share what you’re currently doing or what your new role is, I’d love to hear about it.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 94, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BzqwEboXRqsqIWdnt3ac8gxSadn0l', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c5aceee7-82fe-43fa-8d96-8d3ea914cd1f-0', usage_metadata={'input_tokens': 94, 'output_tokens': 33, 'total_tokens': 127, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'personal_info_detected': 'yes',\n",
       " 'delete_request': 'yes',\n",
       " 'personal_info_extracted': ['Job Title: AI Engineer'],\n",
       " 'new_info': 'yes',\n",
       " 'collected_memories': 'Name: Ashutosh'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Got it, Ashutosh! If you’d like to share what you’re currently doing or what your new role is, I’d love to hear about it.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persistent-memory-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
