{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict, Sequence\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "# from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from services.long_memory_store import MemoryStore\n",
    "\n",
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nbviewer.org/github/Coding-Crashkurse/Long-Term-Memory-Agent/blob/main/longtermmemory.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyInformation(BaseModel):\n",
    "    personal_info: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Indicates whether the information contains any personal user choices or preferences that could help an LLM provide more personalized responses over time for long-term memory use.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = MemoryStore()\n",
    "USER_ID = \"2\"\n",
    "key = \"semantic_memory\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    personal_info_detected: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Indicates whether the information contains any personal user choices or preferences that could help an LLM provide more personalized responses over time for long-term memory use.\"\n",
    "    )\n",
    "    personal_info_extracted: str\n",
    "    new_info: str\n",
    "    collected_memories: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying the inforamtion is  relevancy to stored in long-term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info_classifier(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Classifies if the last user message contains personal info.\n",
    "    Uses few-shot examples to clarify what \"personal info\" is.\n",
    "    \"\"\"\n",
    "    message = state[\"messages\"][-1].content\n",
    "\n",
    "    system_prompt = \"\"\"You are a classifier that checks if a message contains personal info.\n",
    "\n",
    "        Personal info includes:\n",
    "        - Names (e.g., \"John Smith\")\n",
    "        - Locations (e.g., \"Berlin\", \"123 Main St\")\n",
    "        - Preferences or hobbies (e.g., \"I love to code\", \"I prefer short replies\")\n",
    "        - Occupation, phone numbers, or unique IDs\n",
    "\n",
    "        Examples:\n",
    "        User: \"My name is Thomas, I live in Vancouver.\"\n",
    "        Classifier: \"Yes\"\n",
    "\n",
    "        User: \"I love pizza with extra cheese.\"\n",
    "        Classifier: \"Yes\"\n",
    "\n",
    "        User: \"Hello, how are you?\"\n",
    "        Classifier: \"No\"\n",
    "\n",
    "        User: \"This is great weather.\"\n",
    "        Classifier: \"No\"\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{message}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=50)\n",
    "    structured_llm = llm.with_structured_output(ClassifyInformation)\n",
    "    chain = prompt | structured_llm\n",
    "\n",
    "    result = chain.invoke({\"message\": message})\n",
    "    state[\"personal_info_detected\"] = result.personal_info\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the answer of above node is true, we have to extract the informatiom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info_router(state: AgentState) -> Literal[\"extract_personal_info\", \"retrieve_memories\"]:\n",
    "    \"\"\"\n",
    "    If classified 'Yes', go to extraction. Otherwise skip directly to retrieving memories.\n",
    "    \"\"\"\n",
    "    if state[\"personal_info_detected\"].lower() == \"yes\":\n",
    "        return \"extract_personal_info\"\n",
    "    return \"retrieve_memories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info_extractor(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Extracts personal info from the user's message using few-shot examples.\n",
    "    \"\"\"\n",
    "    message = state[\"messages\"][-1].content\n",
    "\n",
    "    # A few-shot style system prompt for extraction:\n",
    "    extractor_system = \"\"\"\n",
    "    You are an intelligent extractor that identifies and summarizes personal user information for long-term memory storage. Your goal is to help the LLM provide more personalized responses in the future.\n",
    "\n",
    "    Instructions:\n",
    "    - Read the user's input carefully.\n",
    "    - Extract any relevant personal details, such as:\n",
    "    - Name, location, profession, and age\n",
    "    - Interests, hobbies, goals, future plans\n",
    "    - Food preferences (likes/dislikes)\n",
    "    - Emotional states, values, or beliefs\n",
    "    - Mention of past experiences (e.g., education, travel)\n",
    "    - Any unique or identifying details\n",
    "\n",
    "    Output Format:\n",
    "    - Return a concise, comma-separated list of attributes in the format: \n",
    "    \"Attribute: Value\"\n",
    "    - If multiple values exist, separate them with commas.\n",
    "    - If no personal info is found, return: \"No personal info found.\"\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    User: \"I am John and I live in Seattle.\"\n",
    "    Output: \"Name: John, Location: Seattle\"\n",
    "\n",
    "    User: \"Hey, I'm Lucy. I love eating bananas!\"\n",
    "    Output: \"Name: Lucy, Food Preference: bananas\"\n",
    "\n",
    "    User: \"I am planning to apply to grad school in Germany next year.\"\n",
    "    Output: \"Goal: apply to grad school, Location: Germany, Timeline: next year\"\n",
    "\n",
    "    User: \"Just a random statement about the weather.\"\n",
    "    Output: \"No personal info found.\"\n",
    "\n",
    "    Now extract personal information from the following user input:\n",
    "    \"\"\"\n",
    "    extractor_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", extractor_system),\n",
    "            (\"human\", \"Input: {message}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=50)\n",
    "    chain = extractor_prompt | llm\n",
    "    extracted_info = chain.invoke({\"message\": message})\n",
    "    print(\"===========\")\n",
    "    print(f\"Personal Info: {extracted_info.content.strip()}\")\n",
    "    print(\"===========\")\n",
    "\n",
    "    state[\"personal_info_extracted\"] = extracted_info.content.strip()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After extracting the information, Let's check for duplimacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNoveltyGrade(BaseModel):\n",
    "    score: Literal[\"yes\", \"no\"] = Field()\n",
    "\n",
    "def personal_info_duplicate_classifier(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Checks if the newly extracted info is already in the store or not.\n",
    "    If 'Yes', it's new info. If 'No', it's a duplicate.\n",
    "    \"\"\"\n",
    "    new_info = state.get(\"personal_info_extracted\", \"\")\n",
    "    namespace = (\"user\", USER_ID)\n",
    "    key = \"semantic_memory\"\n",
    "    results = store.get(namespace, key)\n",
    "    old_info_list = [doc for doc in results]\n",
    "\n",
    "    system_msg = \"\"\"You are a classifier that checks if the new personal info is already stored.\n",
    "If the new info adds anything new, respond 'Yes'. Otherwise 'No'.\"\"\"\n",
    "\n",
    "    old_info_str = \"\\n\".join(old_info_list) if old_info_list else \"No stored info so far.\"\n",
    "    human_template = \"\"\"New info:\\n{new_info}\\n\n",
    "Existing memory:\\n{old_info}\\n\n",
    "Answer ONLY 'Yes' if the new info is unique. Otherwise 'No'.\"\"\"\n",
    "    human_msg = human_template.format(new_info=new_info, old_info=old_info_str)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_msg),\n",
    "            (\"human\", \"{human_msg}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=50).with_structured_output(InfoNoveltyGrade)\n",
    "    chain = prompt | llm\n",
    "    result = chain.invoke({\"human_msg\": human_msg})\n",
    "    state[\"new_info\"] = result.score.strip()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If no dublicate found, add the memory to persistent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info_deduper_router(state: AgentState) -> Literal[\"personal_info_storer\", \"retrieve_memories\"]:\n",
    "    \"\"\"\n",
    "    If 'Yes', store the new info. Otherwise skip storing.\n",
    "    \"\"\"\n",
    "    if state[\"new_info\"].lower() == \"yes\":\n",
    "        return \"personal_info_storer\"\n",
    "    return \"retrieve_memories\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiving the information about the user to call_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_info_storer(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Stores the new personal info in memory if it exists.\n",
    "    \"\"\"\n",
    "    extracted = state.get(\"personal_info_extracted\")\n",
    "    \n",
    "    if extracted:\n",
    "        namespace = (\"user\", USER_ID)\n",
    "        store.put(namespace, key, {\"text\": str(extracted)})\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_memories(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Retrieves all personal info from the store and aggregates into 'collected_memories'.\n",
    "    \"\"\"\n",
    "    results = store.get((\"user\", USER_ID), key)\n",
    "    memory_strs = [doc for doc in results]\n",
    "    state[\"collected_memories\"] = \"\\n\".join(memory_strs)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging the personal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_personal_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Logs the memory to stdout for debugging (optional).\n",
    "    \"\"\"\n",
    "    print(\"----- Logging Personal Memory -----\")\n",
    "    if state[\"collected_memories\"]:\n",
    "        for i, line in enumerate(state[\"collected_memories\"].split(\"\\n\"), start=1):\n",
    "            print(f\"[Memory {i}] {line}\")\n",
    "    else:\n",
    "        print(\"[Memory] No personal info stored yet.\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Final LLM call that uses the collected memories in a SystemMessage.\n",
    "    \"\"\"\n",
    "    personal_info = state.get(\"collected_memories\", \"\")\n",
    "    system_msg = SystemMessage(\n",
    "        content=f\"You are a helpful assistant. The user has shared these personal details:\\n{personal_info}\"\n",
    "    )\n",
    "    all_messages = [system_msg] + list(state[\"messages\"])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=50)\n",
    "    response = llm.invoke(all_messages)\n",
    "    state[\"messages\"] = state[\"messages\"] + [response]\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"personal_info_classifier\", personal_info_classifier)\n",
    "workflow.add_node(\"personal_info_extractor\", personal_info_extractor)\n",
    "workflow.add_node(\"personal_info_duplicate_classifier\", personal_info_duplicate_classifier)\n",
    "workflow.add_node(\"personal_info_storer\", personal_info_storer)\n",
    "workflow.add_node(\"retrieve_memories\", retrieve_memories)\n",
    "workflow.add_node(\"log_personal_memory\", log_personal_memory)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "\n",
    "workflow.add_edge(START, \"personal_info_classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"personal_info_classifier\",\n",
    "    personal_info_router,\n",
    "    {\n",
    "        \"extract_personal_info\": \"personal_info_extractor\",\n",
    "        \"retrieve_memories\": \"retrieve_memories\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"personal_info_extractor\", \"personal_info_duplicate_classifier\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"personal_info_duplicate_classifier\",\n",
    "    personal_info_deduper_router,\n",
    "    {\n",
    "        \"personal_info_storer\": \"personal_info_storer\",\n",
    "        \"retrieve_memories\": \"retrieve_memories\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"personal_info_storer\", \"retrieve_memories\")\n",
    "workflow.add_edge(\"retrieve_memories\", \"log_personal_memory\")\n",
    "workflow.add_edge(\"log_personal_memory\", \"call_model\")\n",
    "workflow.add_edge(\"call_model\", END)\n",
    "\n",
    "workflow.set_entry_point(\"personal_info_classifier\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "# graph = workflow.compile(checkpointer=checkpointer, store=store)\n",
    "graph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "Personal Info: \"Goal: visit New Zealand\"\n",
      "===========\n",
      "----- Logging Personal Memory -----\n",
      "[Memory 1] \"Food Preference: bananas\"\n",
      "[Memory 2] \"Goal: visit New Zealand\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi, I like to eat bananas', additional_kwargs={}, response_metadata={}, id='10c8a914-212a-451c-aba2-359088e8165d'),\n",
       "  AIMessage(content=\"That's great! Bananas are not only delicious but also packed with nutrients. They're a good source of potassium, vitamin C, and dietary fiber. Do you have a favorite way to enjoy bananas?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 38, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BzUYzMc3UO17F6m907TrQIa6ELKKS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--79a933a3-465c-482b-994b-0978cafd7673-0', usage_metadata={'input_tokens': 38, 'output_tokens': 39, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='hi, I want to visit new zealand', additional_kwargs={}, response_metadata={}, id='86b67847-6b0b-4240-b834-45abbfb6ff02'),\n",
       "  AIMessage(content=\"That sounds like an amazing plan! New Zealand is known for its stunning landscapes, from mountains and beaches to lush forests. Are there specific places or activities in New Zealand that you're particularly interested in?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 101, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BzUZTkbpZKTd2qWW3EqfwcKo4YvsM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--42abc87d-9b0b-449e-9ee9-491cd5c28b65-0', usage_metadata={'input_tokens': 101, 'output_tokens': 39, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'personal_info_detected': 'yes',\n",
       " 'personal_info_extracted': '\"Goal: visit New Zealand\"',\n",
       " 'new_info': 'yes',\n",
       " 'collected_memories': '\"Food Preference: bananas\"\\n\"Goal: visit New Zealand\"'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_1 = {\"messages\": [HumanMessage(content=\"hi, I want to visit new zealand\")]}\n",
    "graph.invoke(input=input_data_1, config={\"configurable\": {\"thread_id\": 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_memory = store.get((\"user\", USER_ID), key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Food Preference: bananas\"', '\"Goal: visit New Zealand\"']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persistent-memory-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
